{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bangla Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to normalize bangla text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_bangla_text(text):\n",
    "    # Remove digits\n",
    "    text = re.sub(r'[০-৯]', '', text) \n",
    "    \n",
    "    # Step 1: Remove invisible characters\n",
    "    text = text.replace('\\u200d', '')  # Zero-width joiner\n",
    "    text = text.replace('\\u200c', '')  # Zero-width non-joiner\n",
    "    text = text.replace('\\u00a0', ' ') # Non-breaking space to regular space\n",
    "    \n",
    "    # Step 2: Normalize visually similar characters\n",
    "    text = re.sub(r'[য়]', 'য়', text)   # Normalize 'য়' to 'য়'\n",
    "    text = re.sub(r'[র‍]', 'র', text)   # Remove ZWJ from 'র‍' if used wrongly\n",
    "    text = re.sub(r'[ৎ]', 'ত্', text)   # Rare cases where 'ৎ' needs to be decomposed\n",
    "    text = re.sub(r'[ড়]', 'র়', text)   # Normalize dotted র\n",
    "    text = re.sub(r'[ঢ়]', 'ঢ়', text)   # Normalize dotted ঢ\n",
    "    text = re.sub(r'[ঙ‍]', 'ঙ', text)   # Remove ZWJ after ঙ if it exists\n",
    "\n",
    "    # Step 3: Normalize vowel signs and nukta forms\n",
    "    text = re.sub(r'[\\u09c7\\u09c8]', '\\u09c7', text)  # Normalize e-kar and ai-kar variants\n",
    "    text = re.sub(r'[\\u09cb\\u09cc]', '\\u09cb', text)  # Normalize o-kar and au-kar variants\n",
    "  \n",
    "    # Optional: remove duplicate diacritics (common from faulty OCR or typing)\n",
    "    text = re.sub(r'([ািীুূেৈোৌ])\\1+', r'\\1', text)   # Collapse repeated vowel signs\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'আমি বাংলায়য় বই পডয়তে ভালোবাসি। এটি খুবই আনন্দদায়য়ক! কিন্তু, কিছু বইয়য়ের দাম বেশি। নাহলে আমি  টি বই কিনার ইচ্ছা আসে।'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bangla_text = \"আমি বাংলায় বই পড়তে ভালোবাসি। এটি খুবই আনন্দদায়ক! কিন্তু, কিছু বইয়ের দাম বেশি। নাহলে আমি ৪ টি বই কিনার ইচ্ছা আসে।\"\n",
    "\n",
    "text = normalize_bangla_text(bangla_text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'আমি বাংলায়য় বই পডয়তে ভালোবাসি এটি খুবই আনন্দদায়য়ক কিন্তু কিছু বইয়য়ের দাম বেশি নাহলে আমি  টি বই কিনার ইচ্ছা আসে'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation (including Bangla and English punctuation)\n",
    "punctuation_pattern = r'[^\\w\\s\\u0980-\\u09FF]'  # Retain Bangla and English word characters and whitespace\n",
    "text = re.sub(punctuation_pattern, '', text)\n",
    "text = text.strip()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['আমি', 'বাংলায়য়', 'বই', 'পডয়তে', 'ভালোবাসি', 'এটি', 'খুবই', 'আনন্দদায়য়ক', 'কিন্তু', 'কিছু', 'বইয়য়ের', 'দাম', 'বেশি', 'নাহলে', 'আমি', 'টি', 'বই', 'কিনার', 'ইচ্ছা', 'আসে']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize (split by whitespace)\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['বাংলায়য়', 'বই', 'পডয়তে', 'আনন্দদায়য়ক', 'বইয়য়ের', 'বই', 'ইচ্ছা']\n"
     ]
    }
   ],
   "source": [
    "# Define a basic set of Bangla stopwords\n",
    "bangla_stopwords = set([\n",
    "    'আমি', 'আমরা', 'তুমি', 'তোমরা', 'সে', 'তারা', 'এই', 'ওই', 'এটি', 'ওটি', 'এ', 'ও', 'কিছু', 'কিন্তু', 'আর', 'তবে', 'যে', 'যা', 'তাই', 'এবং', 'বা', 'এর', 'তার', 'এরই', 'তাদের', 'নাহলে', 'টি', 'একটি', 'খুবই', 'ভালো', 'ভালোবাসি', 'আসে', 'হয়', 'হয়', 'দিয়ে', 'দিয়ে', 'করে', 'করেছে', 'করেন', 'করছি', 'করবে', 'করার', 'কিনার', 'পড়তে', 'পড়ে', 'পড়ার', 'দাম', 'বেশি', 'আনন্দ', 'আনন্দদায়ক', 'আনন্দদায়ক', 'ভালোবাসা', 'ভালোবাসি', 'নয়', 'নয়', 'হয়েছে', 'হয়েছে', 'হয়নি', 'হয়নি'\n",
    "])\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_tokens = [token for token in tokens if token not in bangla_stopwords]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize using bnlp\n",
    "\n",
    "Doc: https://github.com/sagorbrur/bnlp/tree/main/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bnlp_toolkit in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (4.0.3)\n",
      "Requirement already satisfied: sentencepiece==0.2.0 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from bnlp_toolkit) (0.2.0)\n",
      "Requirement already satisfied: gensim==4.3.2 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from bnlp_toolkit) (4.3.2)\n",
      "Requirement already satisfied: nltk in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from bnlp_toolkit) (3.9.1)\n",
      "Requirement already satisfied: numpy in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from bnlp_toolkit) (1.26.4)\n",
      "Requirement already satisfied: scipy==1.10.1 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from bnlp_toolkit) (1.10.1)\n",
      "Requirement already satisfied: sklearn-crfsuite==0.3.6 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from bnlp_toolkit) (0.3.6)\n",
      "Requirement already satisfied: tqdm==4.66.3 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from bnlp_toolkit) (4.66.3)\n",
      "Requirement already satisfied: ftfy==6.2.0 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from bnlp_toolkit) (6.2.0)\n",
      "Requirement already satisfied: emoji==1.7.0 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from bnlp_toolkit) (1.7.0)\n",
      "Requirement already satisfied: requests in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from bnlp_toolkit) (2.32.3)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from ftfy==6.2.0->bnlp_toolkit) (0.2.13)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from gensim==4.3.2->bnlp_toolkit) (7.1.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (0.9.11)\n",
      "Requirement already satisfied: six in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (1.17.0)\n",
      "Requirement already satisfied: tabulate in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from sklearn-crfsuite==0.3.6->bnlp_toolkit) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim==4.3.2->bnlp_toolkit) (1.17.2)\n",
      "Requirement already satisfied: click in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from nltk->bnlp_toolkit) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from nltk->bnlp_toolkit) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from nltk->bnlp_toolkit) (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from requests->bnlp_toolkit) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from requests->bnlp_toolkit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from requests->bnlp_toolkit) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages (from requests->bnlp_toolkit) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bnlp_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['আমি', 'বাংলায়', 'গান', 'গাই', '।']\n"
     ]
    }
   ],
   "source": [
    "from bnlp import BasicTokenizer\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "\n",
    "raw_text = \"আমি বাংলায় গান গাই।\"\n",
    "tokens = tokenizer(raw_text)\n",
    "print(tokens)\n",
    "# output: [\"আমি\", \"বাংলায়\", \"গান\", \"গাই\", \"।\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['আমি', 'ভাত', 'খাই', '।', 'সে', 'বাজারে', 'যায়', '।', 'তিনি', 'কি', 'সত্যিই', 'ভালো', 'মানুষ', '?']\n",
      "['আমি ভাত খাই।', 'সে বাজারে যায়।', 'তিনি কি সত্যিই ভালো মানুষ?']\n"
     ]
    }
   ],
   "source": [
    "from bnlp import NLTKTokenizer\n",
    "\n",
    "bnltk = NLTKTokenizer()\n",
    "\n",
    "text = \"আমি ভাত খাই। সে বাজারে যায়। তিনি কি সত্যিই ভালো মানুষ?\"\n",
    "word_tokens = bnltk.word_tokenize(text)\n",
    "sentence_tokens = bnltk.sentence_tokenize(text)\n",
    "print(word_tokens)\n",
    "print(sentence_tokens)\n",
    "# output\n",
    "# word_token: [\"আমি\", \"ভাত\", \"খাই\", \"।\", \"সে\", \"বাজারে\", \"যায়\", \"।\", \"তিনি\", \"কি\", \"সত্যিই\", \"ভালো\", \"মানুষ\", \"?\"]\n",
    "# sentence_token: [\"আমি ভাত খাই।\", \"সে বাজারে যায়।\", \"তিনি কি সত্যিই ভালো মানুষ?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengali SentencePiece Tokenization\n",
    "\n",
    "Tokenization using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁আমি', '▁ভাত', '▁খাই', '।', '▁সে', '▁বাজারে', '▁যায়', '।']\n",
      "[914, 5265, 24224, 3, 124, 2244, 41, 3]\n",
      "আমি ভাত খাই। সে বাজারে যায়।\n"
     ]
    }
   ],
   "source": [
    "from bnlp import SentencepieceTokenizer\n",
    "\n",
    "bsp = SentencepieceTokenizer()\n",
    "\n",
    "\n",
    "input_text = \"আমি ভাত খাই। সে বাজারে যায়।\"\n",
    "tokens = bsp.tokenize(input_text)\n",
    "print(tokens)\n",
    "text2id = bsp.text2id(input_text)\n",
    "print(text2id)\n",
    "id2text = bsp.id2text(text2id)\n",
    "print(id2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuations removed: আমি ভাত খাই সে বাজারে যায়\n",
      "Digits removed: আমি ভাত খাই সে বাজারে যায়\n"
     ]
    }
   ],
   "source": [
    "from bnlp import BengaliCorpus as corpus\n",
    "\n",
    "# Preprocessing Bangla text\n",
    "text = \"আমি ভাত খাই। সে বাজারে যায়।\"\n",
    "\n",
    "# Removing punctuations\n",
    "punctuations_removed = ''.join([char for char in text if char not in corpus.punctuations])\n",
    "print(\"Punctuations removed:\", punctuations_removed)\n",
    "\n",
    "# Removing digits\n",
    "digits_removed = ''.join([char for char in punctuations_removed if char not in corpus.digits])\n",
    "print(\"Digits removed:\", digits_removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['আমি', 'ভাত', 'খাই', 'সে', 'বাজারে', 'যায়']\n"
     ]
    }
   ],
   "source": [
    "from bnlp import NLTKTokenizer\n",
    "\n",
    "bnltk = NLTKTokenizer()\n",
    "\n",
    "word_tokens = bnltk.word_tokenize(digits_removed)\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed: ['ভাত', 'খাই', 'বাজারে', 'যায়']\n"
     ]
    }
   ],
   "source": [
    "# Removing stopwords\n",
    "stopwords_removed = [word for word in word_tokens if word not in corpus.stopwords]\n",
    "print(\"Stopwords removed:\", stopwords_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bangla_stem(word):\n",
    "    suffixes = [\n",
    "        \"গুলো\", \"গুলি\", \"রা\", \"টির\", \"তে\", \"কে\", \"তো\", \"টা\", \"টির\", \"টির\", \"দের\", \"য়ের\",\n",
    "        \"য়\", \"ের\", \"ে\", \"ি\", \"র\", \"টি\", \"টা\", \"েরা\", \"েরা\", \"কে\", \"ও\"\n",
    "    ]\n",
    "\n",
    "    for suffix in sorted(suffixes, key=len, reverse=True):  # Longest match first\n",
    "        if word.endswith(suffix) and len(word) > len(suffix) + 1:\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['আম', 'ভাত', 'খাই', 'সে', 'বাজার', 'যায়']\n"
     ]
    }
   ],
   "source": [
    "print([bangla_stem(word) for word in word_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
