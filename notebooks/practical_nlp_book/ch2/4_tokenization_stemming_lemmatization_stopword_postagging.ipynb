{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, Stemming, Lemmatization and Pos_Tagging using libraries like spacy and nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.12/site-packages (2.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.12/site-packages (from nltk) (1.5.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.2.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Using cached thinc-8.3.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.15.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.12/site-packages (from spacy) (2.2.5)\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m634.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2 (from spacy)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting setuptools (from spacy)\n",
      "  Downloading setuptools-80.8.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.12/site-packages (from spacy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached blis-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting click<8.2,>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/fahad/projects/personal/mlops/ml_fundamentals/venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (227 kB)\n",
      "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached preshed-3.0.9-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.2/444.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic_core-2.33.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Using cached thinc-8.3.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "Using cached typer-0.15.4-py3-none-any.whl (45 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading setuptools-80.8.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached blis-1.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached marisa_trie-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-inspection, spacy-loggers, spacy-legacy, shellingham, setuptools, pydantic-core, murmurhash, mdurl, MarkupSafe, idna, cloudpathlib, click, charset-normalizer, certifi, catalogue, blis, annotated-types, srsly, smart-open, requests, pydantic, preshed, markdown-it-py, marisa-trie, jinja2, rich, language-data, confection, typer, thinc, langcodes, weasel, spacy\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.2.1\n",
      "    Uninstalling click-8.2.1:\n",
      "      Successfully uninstalled click-8.2.1\n",
      "Successfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 certifi-2025.4.26 charset-normalizer-3.4.2 click-8.1.8 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 idna-3.10 jinja2-3.1.6 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 preshed-3.0.9 pydantic-2.11.5 pydantic-core-2.33.2 requests-2.32.3 rich-14.0.0 setuptools-80.8.0 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.15.4 typing-inspection-0.4.1 urllib3-2.4.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install necessary libraries\n",
    "%pip install numpy\n",
    "%pip install nltk\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
    "corpus = \"Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run 4 times !!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower case the corpus\n",
    "corpus = corpus.lower()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run  times !!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing digits using regex\n",
    "import re\n",
    "\n",
    "corpus = re.sub(r'\\d+', '', corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need to finalize the demo corpus which will be used for this notebook  should be done soon  it should be done by the ending of this month but will it this notebook has been run  times '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing punctuations\n",
    "import string\n",
    "\n",
    "corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing trailing whitespaces\n",
    "corpus = ' '.join([token for token in corpus.split()])\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "# ## What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units called tokens. These tokens can be words, sentences, or subwords, depending on the application. Tokenization is a fundamental step in natural language processing (NLP) because it transforms raw text into a format that can be more easily analyzed and processed by algorithms.\n",
    " \n",
    "For example, the sentence \"Natural Language Processing is fun!\" can be tokenized into the words: [\"Natural\", \"Language\", \"Processing\", \"is\", \"fun\", \"!\"].\n",
    "\n",
    "Tokenization helps in tasks such as text analysis, information retrieval, and preparing data for machine learning models.\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Necessray resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Download pre-trained english model for spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/fahad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/fahad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/fahad/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required resources\n",
    "import nltk\n",
    "# Download a list of common stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Download pre-trained tokenizer model\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it', 'mustn', \"that'll\", \"it'll\", 'themselves', \"they've\", 'about', 'them', 'by', \"mightn't\", 'herself', 'me', 'while', 'under', 'further', 'and', 'not', \"haven't\", 'mightn', 'which', 'out', 'my', \"you're\", 'won', 'himself', \"you've\", 'now', \"needn't\", 'its', 'into', 'there', 'being', 'do', 'because', 'don', 'isn', 'needn', 'in', \"mustn't\", \"they're\", 'is', 'more', 'where', 'wasn', \"couldn't\", 'only', 'ours', 'over', 'yours', 'when', 'from', 'very', \"don't\", 'before', \"isn't\", 'to', 'him', 'above', 'ourselves', 'wouldn', 'some', \"he'll\", 'nor', 'the', 'haven', 'on', 'weren', 'these', 'below', 'myself', \"they'd\", 'does', 'until', 'we', 'most', 'through', 'hers', 'our', 'a', 'he', \"we'd\", 'm', 've', 'their', 'down', 'such', 'was', 'but', 'am', \"it's\", \"we're\", 're', \"she'll\", 'if', 'have', 'itself', 'o', 'be', \"hasn't\", 'whom', 'ain', 'again', \"i'll\", 'then', 'theirs', 'having', 't', 'should', \"doesn't\", 'for', 'so', 'that', 'her', 'with', \"he's\", 'hasn', 'aren', 'has', 'shouldn', \"they'll\", 'ma', 'yourselves', 'will', \"hadn't\", 'she', 'those', 'too', 'other', 'each', 'same', 'few', 'you', \"shouldn't\", 'yourself', 'shan', 'your', \"weren't\", \"it'd\", 'or', 'during', 'can', 'how', \"we'll\", \"wouldn't\", 'at', 'didn', 'of', 'are', 'been', 'were', 'between', 'had', 'i', \"she's\", \"didn't\", \"aren't\", 'doesn', 'hadn', 'off', 'here', 'll', 'up', \"he'd\", 'both', 'what', 'this', \"we've\", \"i've\", 'just', \"i'd\", 'who', \"should've\", 'his', 'own', 'did', 'than', 'as', 's', 'an', 'all', 'no', \"shan't\", \"she'd\", 'd', 'against', \"wasn't\", 'why', 'they', 'once', \"won't\", 'doing', \"i'm\", 'couldn', 'y', \"you'd\", 'after', \"you'll\", 'any'}\n"
     ]
    }
   ],
   "source": [
    "# Load english stopwords and create a set to faster lookups\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "print(stop_words_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus_nltk = word_tokenize(corpus)\n",
    "print(tokenized_corpus_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['need',\n",
       " 'finalize',\n",
       " 'demo',\n",
       " 'corpus',\n",
       " 'used',\n",
       " 'notebook',\n",
       " 'done',\n",
       " 'soon',\n",
       " 'done',\n",
       " 'ending',\n",
       " 'month',\n",
       " 'notebook',\n",
       " 'run',\n",
       " 'times']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out tokens that are in stopwords\n",
    "tokenized_corpus_nltk_without_stopwords = [token for token in tokenized_corpus_nltk if token.lower() not in stop_words_nltk]\n",
    "\n",
    "tokenized_corpus_nltk_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spacy and load its english model\n",
    "import spacy\n",
    "spacy_model = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'’s', 'about', 'wherever', 'elsewhere', 'my', \"'ve\", 'now', 'there', 'do', 'thru', 'either', 'because', 'give', 'might', '’m', 'without', 'only', 'over', 'regarding', 'when', 'from', 'very', '’ll', 'before', 'below', 'front', 'does', 'sometimes', 'we', 'most', 'a', 'someone', 'much', '’d', 'somewhere', 'whatever', 'via', 'somehow', 'nine', 'have', 'again', 'part', 'then', 'already', 'with', 'another', 'third', 'she', \"'s\", 'whole', 'each', 'same', 'yourself', 'whenever', 'therefore', 'beforehand', 'during', 'one', 'almost', 'are', 'twenty', 'whether', 'alone', 'ever', 'two', 'ten', 'up', 'thereafter', 'except', 'than', 'all', 'indeed', 'once', '‘s', 'upon', 'various', 'may', 'mostly', 'herself', 'me', 'latter', 'seemed', 'out', 'besides', 'himself', 'anywhere', 'everything', 'moreover', 'nevertheless', 'together', 'would', 'per', 'three', 'noone', 'twelve', 'everywhere', 'seeming', 'always', 'myself', 'done', 'toward', 'hers', 'our', 'down', 'if', \"'d\", '’ve', 'n‘t', 'that', 'namely', 'well', 'fifteen', 'enough', 'among', 'yourselves', 'will', 'every', 'next', 'due', 'other', 'less', '‘m', 'many', 'at', 'sixty', 'between', 'anything', 'i', 'still', 'full', 'here', 'move', 'formerly', 'ca', 'both', 'this', 'never', 'who', 'first', 'bottom', 'own', 'as', '‘re', 'unless', 'an', 'make', 'seem', 'amount', 'why', 'doing', 'empty', 'though', 'afterwards', 'it', 'mine', 'show', 'herein', 'therein', 'whereby', 'anyway', 'becoming', 'and', 'everyone', 'which', 'please', 'often', 'although', 'fifty', 'whereafter', 'something', 'forty', 'get', 'neither', 'take', 'thus', 'whereas', 'even', 'yours', 'him', 'perhaps', 'ourselves', 'nor', 'must', 'former', 'hence', '‘ve', '’re', 'these', 'whither', 'using', 'he', 'their', 'quite', 'four', 'was', 'but', 'am', 're', '‘ll', 'hundred', 'become', 'top', 'whom', 'for', 'eight', 'so', 'her', 'has', 'us', 'rather', 'however', 'meanwhile', 'too', 'few', 'your', 'anyone', 'say', 'of', 'off', 'whose', 'least', \"'m\", 'name', 'did', 'could', 'no', 'made', 'seems', 'against', 'yet', 'put', 'after', 'along', 'towards', 'themselves', 'go', 'them', 'by', 'while', 'under', 'further', 'not', 'nowhere', 'none', 'its', 'into', 'being', 'last', 'in', 'since', 'call', 'is', 'more', 'where', 'also', 'ours', 'back', 'serious', 'to', 'above', 'nobody', 'beyond', 'some', 'the', 'on', 'behind', 'cannot', 'until', 'whoever', 'through', 'n’t', 'nothing', 'across', 'else', 'wherein', 'such', \"'re\", 'eleven', 'anyhow', 'throughout', 'itself', 'within', 'be', 'several', 'latterly', 'hereafter', 'should', 'whence', 'beside', 'thereby', 'five', 'side', 'used', 'otherwise', 'those', 'whereupon', 'you', 'thereupon', 'or', 'can', 'how', 'around', 'onto', 'been', 'were', '‘d', 'had', 'thence', \"n't\", 'hereby', 'six', 'hereupon', 'others', 'becomes', \"'ll\", 'what', 'just', 'his', 'became', 'really', 'they', 'sometime', 'amongst', 'see', 'keep', 'any'}\n"
     ]
    }
   ],
   "source": [
    "# Stopwords in spacy\n",
    "stopwords_spacy = spacy_model.Defaults.stop_words\n",
    "print(stopwords_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spacy:\n",
      "Tokenized Corpus: need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times\n"
     ]
    }
   ],
   "source": [
    "# Tokenize corpus using spacy\n",
    "\n",
    "print(\"\\nSpacy:\")\n",
    "tokenized_corpus_spacy = spacy_model(corpus)\n",
    "print(\"Tokenized Corpus:\", tokenized_corpus_spacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[need, to, finalize, the, demo, corpus, which, will, be, used, for, this, notebook, should, be, done, soon, it, should, be, done, by, the, ending, of, this, month, but, will, it, this, notebook, has, been, run, times]\n"
     ]
    }
   ],
   "source": [
    "# tokens without stopwords\n",
    "tokens_without_sw = [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
    "print(tokens_without_sw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between nltk and spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in Spacy but not in NLTK: {'it', 'this', 'but', 'of', 'which', 'been', 'will', 'the', 'by', 'has', 'to', 'should', 'for', 'be'}\n",
      "Tokens in NLTK but not in Spacy: set()\n"
     ]
    }
   ],
   "source": [
    "# Find the difference between the two lists (tokens without stopwords from spacy and nltk)\n",
    "tokens_spacy_set = set([token.text.lower() for token in tokens_without_sw])\n",
    "tokens_nltk_set = set([token.lower() for token in tokenized_corpus_nltk_without_stopwords])\n",
    "\n",
    "diff_spacy_nltk = tokens_spacy_set - tokens_nltk_set\n",
    "diff_nltk_spacy = tokens_nltk_set - tokens_spacy_set\n",
    "\n",
    "print(\"Tokens in Spacy but not in NLTK:\", diff_spacy_nltk)\n",
    "print(\"Tokens in NLTK but not in Spacy:\", diff_nltk_spacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Stemming is a text normalization technique in Natural Language Processing (NLP) that reduces words to their root or base form. \n",
    "\n",
    "The root form, called a \"stem,\" may not always be a valid word, but it represents related words with similar meanings. \n",
    "\n",
    "For example, \"running\", \"runs\", and \"ran\" can all be reduced to the stem \"run\". \n",
    "\n",
    "Stemming helps in grouping similar words together, which is useful for tasks like information retrieval and text analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stemming:\n",
      " need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times\n",
      "After Stemming:\n",
      "need to final the demo corpu which will be use for thi notebook should be done soon it should be done by the end of thi month but will it thi notebook ha been run time "
     ]
    }
   ],
   "source": [
    "# Stem using port stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(\"Before Stemming:\\n\", corpus)\n",
    "\n",
    "print('After Stemming:')\n",
    "\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(stemmer.stem(word), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Lemmatization is a text normalization process in Natural Language Processing (NLP) that reduces words to their base or dictionary form, known as a \"lemma.\"\n",
    "\n",
    "Unlike stemming, which may simply chop off word endings and can result in non-words, lemmatization uses vocabulary and morphological analysis to return valid words.\n",
    "\n",
    "For example, the words \"running\", \"ran\", and \"runs\" are all reduced to the lemma \"run\".\n",
    "\n",
    "Lemmatization helps in grouping together different inflected forms of a word so they can be analyzed as a single item, which is useful for tasks like information retrieval, text analysis, and machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/fahad/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use wordnetlemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook ha been run time "
     ]
    }
   ],
   "source": [
    "lemmetizer = WordNetLemmatizer()\n",
    "\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(lemmetizer.lemmatize(word), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging\n",
    " \n",
    "POS (Part-of-Speech) Tagging is the process of labeling each word in a sentence with its appropriate part of speech, such as noun, verb, adjective, etc.\n",
    "\n",
    "This is a fundamental step in many NLP tasks, as it provides information about the grammatical structure of a sentence and the relationships between words.\n",
    "\n",
    "For example, in the sentence \"The quick brown fox jumps over the lazy dog\", POS tagging would identify \"fox\" and \"dog\" as nouns, \"jumps\" as a verb, and \"quick\", \"brown\", \"lazy\" as adjectives.\n",
    " \n",
    "POS tagging can be performed using rule-based, statistical, or machine learning approaches. In Python, the NLTK library provides tools for POS tagging.\n",
    " \n",
    "# Let's see how to perform POS tagging using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS tagging using spacy\n",
    "\n",
    "doc = spacy_model(corpus_original)\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need : VERB\n",
      "to : PART\n",
      "finalize : VERB\n",
      "the : DET\n",
      "demo : NOUN\n",
      "corpus : X\n",
      "which : PRON\n",
      "will : AUX\n",
      "be : AUX\n",
      "used : VERB\n",
      "for : ADP\n",
      "this : DET\n",
      "notebook : NOUN\n",
      "and : CCONJ\n",
      "it : PRON\n",
      "should : AUX\n",
      "be : AUX\n",
      "done : VERB\n",
      "soon : ADV\n",
      "! : PUNCT\n",
      "! : PUNCT\n",
      ". : PUNCT\n",
      "It : PRON\n",
      "should : AUX\n",
      "be : AUX\n",
      "done : VERB\n",
      "by : ADP\n",
      "the : DET\n",
      "ending : NOUN\n",
      "of : ADP\n",
      "this : DET\n",
      "month : NOUN\n",
      ". : PUNCT\n",
      "But : CCONJ\n",
      "will : AUX\n",
      "it : PRON\n",
      "? : PUNCT\n",
      "This : DET\n",
      "notebook : NOUN\n",
      "has : AUX\n",
      "been : AUX\n",
      "run : VERB\n",
      "4 : NUM\n",
      "times : NOUN\n",
      "! : PUNCT\n",
      "! : PUNCT\n"
     ]
    }
   ],
   "source": [
    "# Token and tag\n",
    "for token in doc:\n",
    "    print(token, ':', token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/fahad/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS tagging using nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Need', 'NN'),\n",
      " ('to', 'TO'),\n",
      " ('finalize', 'VB'),\n",
      " ('the', 'DT'),\n",
      " ('demo', 'NN'),\n",
      " ('corpus', 'NN'),\n",
      " ('which', 'WDT'),\n",
      " ('will', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('used', 'VBN'),\n",
      " ('for', 'IN'),\n",
      " ('this', 'DT'),\n",
      " ('notebook', 'NN'),\n",
      " ('and', 'CC'),\n",
      " ('it', 'PRP'),\n",
      " ('should', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('done', 'VBN'),\n",
      " ('soon', 'RB'),\n",
      " ('!', '.'),\n",
      " ('!', '.'),\n",
      " ('.', '.'),\n",
      " ('It', 'PRP'),\n",
      " ('should', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('done', 'VBN'),\n",
      " ('by', 'IN'),\n",
      " ('the', 'DT'),\n",
      " ('ending', 'VBG'),\n",
      " ('of', 'IN'),\n",
      " ('this', 'DT'),\n",
      " ('month', 'NN'),\n",
      " ('.', '.'),\n",
      " ('But', 'CC'),\n",
      " ('will', 'MD'),\n",
      " ('it', 'PRP'),\n",
      " ('?', '.'),\n",
      " ('This', 'DT'),\n",
      " ('notebook', 'NN'),\n",
      " ('has', 'VBZ'),\n",
      " ('been', 'VBN'),\n",
      " ('run', 'VBN'),\n",
      " ('4', 'CD'),\n",
      " ('times', 'NNS'),\n",
      " ('!', '.'),\n",
      " ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(nltk.pos_tag(word_tokenize(corpus_original)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
